{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping \n",
    "\n",
    "Web scraping of the series \"The Big Bang Theory\"\n",
    "\n",
    "Katerina Kashchenko"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web scraping of the series The Big Bang Theory (from here https://bigbangtrans.wordpress.com ). \n",
    "We look at the code of the page with the transcript of the first series https://bigbangtrans.wordpress.com/series-1-episode-1-pilot-episode /. \n",
    "\n",
    "The transcript of the series itself - it is contained under the div tag, the attribute class=\"entry text\" \n",
    "\n",
    "Pulling out the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The requests package is usually used to get data from web pages\n",
    "# import it\n",
    "\n",
    "import requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# How to use this package to send requests and receive information from web pages?\n",
    "# read the package documentation, find the right command https://docs.python-requests.org/en/master/\n",
    "\n",
    "# let's write a link to the page we need to get into the variable\n",
    "\n",
    "url = \"https://bigbangtrans.wordpress.com/series-1-episode-1-pilot-episode/\"  \n",
    "\n",
    "# creating a request to get this page, passing the\n",
    "response = request.get(url) link as a parameter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sometimes the site requires you to register details about the user agent, i.e. the browser from which the request is made*   \n",
    "*In this case, you can specify any data, for example, such*\n",
    "\n",
    "response = request.get(url, header={\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (HTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the entire content of the page is written to the response variable\n",
    "# we can view it using this command --\n",
    "response.text \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got about the same thing as viewing the page code in the browser.\n",
    "It is impossible to understand what is happening here. \n",
    "\n",
    "We need a parser - a set of commands that can be used to separate the code (tags, attributes) from everything else, and get the necessary data.\n",
    "\n",
    "We will use the html parser from the BeautifulSoup package. Documentation - https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's parse the resulting page using a parser\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser') \n",
    "\n",
    "# # The same page is now written to the soup variable, but in a more structured form\n",
    "# # now we can get what we need by tags using the soup method.find All(tag)\n",
    "# find, for example, all links by tag \"a\"\n",
    "soup.findAll('a') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we looked at the page code in the browser, we found out that the text we need is contained under the div tag with the attribute class = \"entrytext\"\n",
    "# We prescribe this tag and attribute\n",
    "# we learned exactly how this string is written from the BeautifulSoup documentation\n",
    "\n",
    "soup.findAll('div', {'class': 'entrytext'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a structure similar to a list. From each element we now need to get the text. We learned from the documentation that this can be done using the get_text() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "for div in soup.findAll('div', {'class': 'entrytext'}):\n",
    "        t = div.get_text() \n",
    "        lines.append(t.strip())      \n",
    "\n",
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already received almost what we need! \n",
    "However, at the end there is some technical text that we don't need. Let's delete it. There are different ways to do this, but we will use regular expressions (https://docs.python.org/3/howto/regex.html ), because on different pages of this site, this text probably looks a little different. We need to delete everything that starts with \"\\__ATA\" and ends with \":Like Loading...\". In the regular expression language, the text can be encoded like this: \\__ATA, any character (.) any number of times (*), :Like Loading...\n",
    "\n",
    "You just need to take into account that in this fragment there are end-of-line icons (\\n). \"Any sign and end-of-line sign\" in the regular expression language does not look like \".\", but like (.|\\s)\n",
    "\n",
    "Therefore, the final regular expression is -- \"__ATA(.|\\s)*:Like Loading...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    " \n",
    "lines = re.sub(\"__ATA(.|\\s)*:Like Loading...\",\"\",lines[0])\n",
    "\n",
    "# let's break the whole text into replicas\n",
    "lines = lines.split('\\n')\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the result to a txt file\n",
    "with open(\"s1e1.txt\", \"w\", encoding=\"utf-8\") as text_file:\n",
    "    for i in lines:\n",
    "        text_file.write(i + '\\n') \n",
    "# Add an end-of-line sign so that we have all replicas on a separate line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we downloaded one transcript. But we need everyone. To do this, you need to somehow get all the links to the pages. There is no one universal method here - you need to look at how the links look. With blog platforms like LiveJournal, everything is simple - there links have the form \"https://username.livejournal.com/?skip=0 \", \"https://username.livejournal.com/?skip=10 \", \"https://username.livejournal.com/?skip=20 and so on. You can generate link texts by changing only the number at the end. In the case of our site, the links look different, it is impossible to generate so easily (the name of the series changes at the end). But on the side there is a Pages section with a list of all the series with links to them. We are now interested not in the displayed text, but in the link.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the usual <\\a> tag for storing links may be suitable. The link text itself is stored with the href attribute (you can find out about this if you read about the html language, for example, here http://htmlbook.ru /, or if you google how to get the link text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download links\n",
    "\n",
    "links_with_text = []\n",
    "\n",
    "links = soup.findAll('a', href=True)\n",
    "for link in links:\n",
    "    if link.text: \n",
    "        links_with_text.append(link['href'])\n",
    "            \n",
    "\n",
    "links_with_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's see what happened - we see that there is something superfluous - 1-4 links and we don't need the last two links\n",
    "# Delete them\n",
    "\n",
    "del links_with_text[0:4]\n",
    "del links_with_text[-2:]\n",
    "\n",
    "links_with_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the part of the link containing the season/episode number and the series name to save files with theseименами \n",
    "titles = []\n",
    "\n",
    "for i in range(len(links_with_text)):\n",
    "    a=links_with_text[i].replace('https://bigbangtrans.wordpress.com/',\"\")\n",
    "    a=a.rstrip(\"/\")\n",
    "    titles.append(a)\n",
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now everything is ready to perform all the necessary operations for all pages\n",
    "\n",
    "for i, url in enumerate(links_with_text):\n",
    "    print(url) \n",
    "    response = requests.get(url,headers={\"User-Agent\" : \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\"})\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    lines = []\n",
    "    for div in soup.findAll('div', {'class': 'entrytext'}):\n",
    "            t = div.get_text() \n",
    "            lines.append(t.strip())      \n",
    "\n",
    "    lines = re.sub(\"__ATA(.|\\s)*:Like Loading...\", \"\", lines[0])\n",
    "    lines = lines.split(\"\\n\")   \n",
    "    # сохраним в txt файл\n",
    "    with open(titles[i]+\".txt\", \"w\", encoding=\"utf-8\") as text_file: \n",
    "        for i in lines:\n",
    "            text_file.write(i + '\\n') \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can make a separate file for each season, so that it is convenient to analyze the transcript texts by season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seasons = {}\n",
    "\n",
    "for i, url in enumerate(links_with_text):\n",
    "    \n",
    "    response = requests.get(url,headers={\"User-Agent\" : \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\"})\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    lines = []\n",
    "    for div in soup.findAll('div', {'class': 'entrytext'}):\n",
    "            t = div.get_text() \n",
    "            lines.append(t.strip())      \n",
    "\n",
    "    lines = re.sub(\"__ATA(.|\\s)*:Like Loading...\", \"\", lines[0])\n",
    "    lines = lines.split(\"\\n\")\n",
    "    \n",
    "# now we check which season the page belongs to, and write everything down in the list,    for j in range(1,11):\n",
    "        series = \"series-\" + str(j) + \"-\"\n",
    "        series0 = \"series-0\" + str(j) + \"-\" # sometimes the links look like series-6-episode-,\n",
    "#and sometimes series-06-episode-, so we add this condition\n",
    "        if series in url or series0 in url:\n",
    "            if j in seasons.keys():\n",
    "                seasons[j].append(lines)\n",
    "            else:\n",
    "                seasons[j] = [lines]\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,11):\n",
    "    seasons[i] = [item for sublist in seasons[i] for item in sublist] # making one list from a list of lists\n",
    "    with open(\"season\"+str(i)+\".txt\", \"w\", encoding=\"utf-8\") as text_file: \n",
    "            for j in seasons[i]:\n",
    "                text_file.write(j + '\\n') # Adding an end-of-line sign so that we have all the replicas on a separate line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We open several files just in case. Everything worked out, hooray!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
